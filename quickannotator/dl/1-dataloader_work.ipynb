{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sqlalchemy import create_engine, event, Table\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "import shapely.wkb\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from sqlalchemy import inspect\n",
    "\n",
    "\n",
    "from quickannotator.db import db_session, Project, Image, AnnotationClass, Notification, Tile, Setting, Annotation, SearchCache, build_annotation_table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"sqlite:////opt/QuickAnnotator/quickannotator/instance/quickannotator.db\"\n",
    "engine = create_engine(db_path)#,echo=True)\n",
    "\n",
    "# Initialize Spatialite extension\n",
    "@event.listens_for(engine, \"connect\")\n",
    "def connect(dbapi_connection, connection_record):\n",
    "    dbapi_connection.enable_load_extension(True)\n",
    "    dbapi_connection.execute('SELECT load_extension(\"mod_spatialite\")')\n",
    "    dbapi_connection.execute('SELECT InitSpatialMetaData(1);')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [Image, AnnotationClass, Tile]\n",
    "db.metadata.create_all(bind=engine, tables=[item.__table__ for item in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all annotation classes\n",
    "annotation_classes = session.query(AnnotationClass).all()\n",
    "print(annotation_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_id = 2\n",
    "\n",
    "# Query all tiles for the current class\n",
    "tiles = session.query(Tile).filter_by(annotation_class_id=class_id).all() ## filter by having a gt=True attribute\n",
    "print(tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openslide\n",
    "from sqlalchemy import inspect\n",
    "\n",
    "for tile in tiles:\n",
    "    image_id = tile.image_id\n",
    "    gtpred = 'gt'  # or 'pred' based on your requirement\n",
    "    table_name = build_annotation_table_name(image_id, class_id, gtpred == 'gt')\n",
    "\n",
    "    # Check if the table exists\n",
    "    inspector = inspect(engine)\n",
    "    if not inspector.has_table(table_name):\n",
    "        continue\n",
    "\n",
    "    table = Table(table_name, db.metadata, autoload_with=engine)\n",
    "\n",
    "    annotations = session.query(table).filter(\n",
    "        table.c.polygon.ST_Within(tile.geom)\n",
    "    ).all()\n",
    "    \n",
    "    \n",
    "    if len(annotations) == 0:\n",
    "        continue\n",
    "\n",
    "    print(\"non zero\", len(annotations))\n",
    "    tpoly=shapely.wkb.loads(tile.geom.data)\n",
    "\n",
    "    # Get the bounds of the polygon\n",
    "    minx, miny, maxx, maxy = tpoly.bounds\n",
    "\n",
    "    # Compute the width and height\n",
    "    width = int(maxx - minx)\n",
    "    height = int(maxy - miny)\n",
    "\n",
    "\n",
    "    #--- image work\n",
    "    image = session.query(Image).filter_by(id=image_id).first()\n",
    "    if not image:\n",
    "        continue\n",
    "\n",
    "    image_path = image.path\n",
    "    print(image_path)\n",
    "    slide = openslide.OpenSlide(\"../\"+image_path)\n",
    "\n",
    "    # Extract the region defined by the tile\n",
    "    region = slide.read_region((int(minx), int(miny)), 0, (width, height))\n",
    "\n",
    "    io_filename = f\"io_{tile.id}.png\"\n",
    "    region.save(io_filename)\n",
    "\n",
    "    print(f\"Width: {width}, Height: {height}\")\n",
    "\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    for annotation in annotations:\n",
    "        annotation_polygon = shapely.wkb.loads(annotation.polygon.data)\n",
    "       \n",
    "        # Translate the annotation polygon to the tile's coordinate system\n",
    "        translated_polygon = shapely.affinity.translate(annotation_polygon, xoff=-minx, yoff=-miny)\n",
    "        \n",
    "        # Draw the translated polygon on the mask\n",
    "        cv2.fillPoly(mask, [np.array(translated_polygon.exterior.coords, dtype=np.int32)], 1)\n",
    "    \n",
    "    \n",
    "    mask_filename = f\"mask_{tile.id}.png\"\n",
    "    cv2.imwrite(mask_filename, mask)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "num=324\n",
    "# Load images\n",
    "io_image = mpimg.imread(f'io_{num}.png')\n",
    "mask_image = mpimg.imread(f'mask_{num}.png')\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Display the images\n",
    "axes[0].imshow(io_image)\n",
    "axes[0].set_title('IO Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(mask_image, cmap='gray')\n",
    "axes[1].set_title('Mask Image')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openslide\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "import numpy as np\n",
    "from PIL import Image as PILImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class TileDataset(IterableDataset):\n",
    "    def __init__(self, tiles, transform=None):\n",
    "        self.tiles = tiles\n",
    "        self.cache = {} #to be convered to memcached\n",
    "        self.transform = transform\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        inspector = inspect(engine)\n",
    "        print(self.cache.keys(),self.tiles)\n",
    "        for tile in self.tiles:\n",
    "            image_id = tile.image_id\n",
    "            tile_id = tile.id\n",
    "            cache_key = f\"{image_id}_{tile_id}\"\n",
    "\n",
    "            if cache_key in self.cache:\n",
    "                print(\"cache hit\")\n",
    "                io_image, mask_image = self.cache[cache_key]\n",
    "            else:\n",
    "            \n",
    "                image_id = tile.image_id\n",
    "                gtpred = 'gt'  # or 'pred' based on your requirement\n",
    "                table_name = build_annotation_table_name(image_id, class_id, gtpred == 'gt')\n",
    "                \n",
    "                if not inspector.has_table(table_name):\n",
    "                    continue\n",
    "                \n",
    "                table = Table(table_name, db.metadata, autoload_with=engine)\n",
    "\n",
    "                \n",
    "                annotations = session.query(table).filter(\n",
    "                    table.c.polygon.ST_Within(tile.geom)\n",
    "                ).all()\n",
    "\n",
    "                \n",
    "                if len(annotations) == 0:\n",
    "                    continue\n",
    "                \n",
    "                tpoly = shapely.wkb.loads(tile.geom.data)\n",
    "\n",
    "                # Get the bounds of the polygon\n",
    "                minx, miny, maxx, maxy = tpoly.bounds\n",
    "\n",
    "                # Compute the width and height\n",
    "                width = int(maxx - minx)\n",
    "                height = int(maxy - miny)\n",
    "\n",
    "                #--- image work\n",
    "                image = session.query(Image).filter_by(id=image_id).first()\n",
    "                if not image:\n",
    "                    continue\n",
    "\n",
    "                image_path = image.path\n",
    "                slide = openslide.OpenSlide(\"../\" + image_path)\n",
    "\n",
    "                # Extract the region defined by the tile\n",
    "                region = slide.read_region((int(minx), int(miny)), 0, (width, height))\n",
    "                io_image = region.convert(\"RGB\")\n",
    "\n",
    "                mask_image = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "                for annotation in annotations:\n",
    "                    annotation_polygon = shapely.wkb.loads(annotation.polygon.data)\n",
    "                    translated_polygon = shapely.affinity.translate(annotation_polygon, xoff=-minx, yoff=-miny)\n",
    "                    cv2.fillPoly(mask_image, [np.array(translated_polygon.exterior.coords, dtype=np.int32)], 1)\n",
    "\n",
    "                # Convert PIL images to tensors\n",
    "                io_image = transforms.ToTensor()(io_image)\n",
    "#                mask_image = transforms.ToTensor()(mask_image)\n",
    "\n",
    "                if self.transform:\n",
    "                    print(\"MASK NOT TRANSFORMED\")\n",
    "                    io_image = self.transform(io_image)\n",
    " #                   mask_image = self.transform(|mask_image)\n",
    "                \n",
    "                self.cache[cache_key] = (io_image, mask_image)\n",
    "\n",
    "            yield io_image, mask_image\n",
    "\n",
    "# Example usage\n",
    "transform = None  # Define any transformations if needed\n",
    "dataset = TileDataset(tiles, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False) #,num_workers=4)\n",
    "\n",
    "# Iterate through the dataloader\n",
    "for images, masks in dataloader:\n",
    "    print(images.shape, masks.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.cache.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Iterate through the batch of images and masks\n",
    "for i in range(images.shape[0]):\n",
    "    io_image = images[i].permute(1, 2, 0).numpy()  # Shift the channel to the end\n",
    "    mask_image = masks[i].squeeze().numpy()  # Remove the channel dimension for mask\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Display the images\n",
    "    axes[0].imshow(io_image)\n",
    "    axes[0].set_title('IO Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(mask_image, cmap='gray')\n",
    "    axes[1].set_title('Mask Image')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import segmentation_models_pytorch as smp\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # Define the model\n",
    "# model = smp.Unet(encoder_name=\"timm-mobilenetv3_small_100\", encoder_weights=\"imagenet\", in_channels=3, classes=1)\n",
    "\n",
    "# # Define the loss function and optimizer\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Move the model to GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)\n",
    "\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 10\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for images, masks in tqdm(dataloader):\n",
    "#         print(\"done loading batch\")\n",
    "#         images = images.to(device)\n",
    "#         masks = masks.to(device)\n",
    "\n",
    "#         # Zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         print(\"doing forward pass\")\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, masks)\n",
    "#         print(\"loss\",loss)\n",
    "#         # Backward pass and optimize\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader)}\")\n",
    "\n",
    "# print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the model\n",
    "model = smp.Unet(encoder_name=\"timm-mobilenetv3_small_100\", encoder_weights=\"imagenet\", in_channels=3, classes=1)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, masks in tqdm(dataloader):\n",
    "        print(\"done loading batch\")\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        print(\"doing forward pass\")\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, masks) \n",
    "\n",
    "        # Mask for positives and unlabeled\n",
    "        positive_mask = (masks == 1).float()\n",
    "        unlabeled_mask = (masks == 0).float()\n",
    "\n",
    "        # Weighted loss\n",
    "        positive_loss = 1.0  * (loss * positive_mask).mean()\n",
    "        unlabeled_loss = .1* (loss * unlabeled_mask).mean()\n",
    "\n",
    "        loss_total  =  positive_loss + unlabeled_loss\n",
    "\n",
    "        \n",
    "        print(\"losses:\\t\",loss_total,positive_mask.sum(),positive_loss,unlabeled_loss)\n",
    "        # Backward pass and optimize\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss_total.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader)}\")\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Iterate through the batch of images and masks\n",
    "for i in range(images.shape[0]):\n",
    "    io_image = images[i].permute(1, 2, 0).numpy()  # Shift the channel to the end\n",
    "    mask_image = masks[i].squeeze().numpy()  # Remove the channel dimension for mask\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Display the images\n",
    "    axes[0].imshow(io_image)\n",
    "    axes[0].set_title('IO Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(mask_image, cmap='gray')\n",
    "    axes[1].set_title('Mask Image')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(mask_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
