{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ray[train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U ipywidgets\n",
    "# not needed for production, only for jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- need to run \n",
    "#memcached -m 10240 -I 20m -vv -u www-data &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- should clean imports - copy paste\n",
    "import ray\n",
    "from ray.train import ScalingConfig\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import ray.train.torch\n",
    "import ray.util.state\n",
    "import ray\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import ray.train.torch\n",
    "import ray.train\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.optim import Adam\n",
    "from quickannotator.constants import TileStatus\n",
    "from quickannotator.db import get_session\n",
    "from quickannotator.db.models import Tile\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=68566)\u001b[0m losses:\t tensor(0.0040, device='cuda:0', grad_fn=<AddBackward0>) tensor(469940., device='cuda:0') tensor(0.0022, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0018, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\u001b[36m(RayTrainWorker pid=68566)\u001b[0m ntiles 233\n",
      "\u001b[36m(RayTrainWorker pid=68566)\u001b[0m <quickannotator.db.models.Tile object at 0x7fbf071f7c70>\n",
      "\u001b[36m(RayTrainWorker pid=68566)\u001b[0m 233 1 614 45056 16384 2048 150784 71936\n",
      "\u001b[36m(RayTrainWorker pid=68566)\u001b[0m shapes! (2048, 2048, 3) (2048, 2048) (2048, 2048)\n"
     ]
    }
   ],
   "source": [
    "ray.init( namespace=\"quick_annotator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quickannotator.dl.training import train_pred_loop\n",
    "# https://discuss.ray.io/t/core-how-to-make-sure-an-actor-is-initialized/1537/5 might be worth keeping \n",
    "\n",
    "@ray.remote\n",
    "class DLActor:\n",
    "    def __init__(self,actor_name,classid,tile_size,magnification):\n",
    "        \n",
    "        print(f\"{actor_name=}\")\n",
    "        print(f\"{classid=}\")\n",
    "        print(f\"{tile_size=}\")\n",
    "        self.actor_name=actor_name #TODO: i don't know if there is a way to get this reflexively instead of passing it in? -- trying to get something working first : ) \n",
    "        self.classid=classid\n",
    "        self.tile_size=tile_size\n",
    "        self.enable_training=False # starts not training\n",
    "        self.closedown = False \n",
    "        self.allow_pred=True # --- don't know if we'll ever need this, so hard setting to true\n",
    "        self.hexid=None\n",
    "        self.magnification = magnification\n",
    "\n",
    "    def start_dlproc(self,allow_pred=True):\n",
    "        #--------- this is a bit of a disaster - need to basically make sure we're not launching multple\n",
    "        #TODO: check if hexid is alive and valid -- can't start two trainings!\n",
    "        # if is_training(self.hexid):\n",
    "        #     return False\n",
    "        #need to check if is already training, if yes, no opt\n",
    "        print(\"starting up\", self.actor_name)\n",
    "        #---------------------------\n",
    "\n",
    "        #scaling_config = ray.train.ScalingConfig(num_workers=2, use_gpu=True,resources_per_worker={\"GPU\":.5})\n",
    "        ## will fail on single node single gpu --- (DLActor pid=174521) Duplicate GPU detected : rank 1 and rank 0 both on CUDA device 1000\n",
    "\n",
    "        #--- NEED A GPU CONTAINER TO TEST THIS        \n",
    "        total_gpus  = ray.cluster_resources().get(\"GPU\", 0)\n",
    "        # total_gpus_available=  ray.available_resources().get(\"GPU\", 0)\n",
    "        # print(total_gpus,total_gpus_available)\n",
    "        # resource_to_request = (total_gpus_available/total_gpus)/2 * 1.01\n",
    "        # print(f\"resource_to_request: {resource_to_request}\")\n",
    "\n",
    "        scaling_config = ray.train.ScalingConfig(num_workers=int(total_gpus), use_gpu=True,\n",
    "                                                resources_per_worker={\"GPU\":.01}, placement_strategy=\"STRICT_SPREAD\")\n",
    "        \n",
    "        #----\n",
    "#        scaling_config = ray.train.ScalingConfig(num_workers=1, use_gpu=False)\n",
    "        trainer = ray.train.torch.TorchTrainer(train_pred_loop,\n",
    "                                       scaling_config=scaling_config,\n",
    "                                       train_loop_config={'actor_name':self.actor_name,\n",
    "                                                          'classid':self.classid,\n",
    "                                                         'tile_size':self.tile_size,\n",
    "                                                         'magnification':self.magnification})\n",
    "        self.hexid = trainer.fit().hex() ##widly -- this doesn't save the result in the actor\n",
    "        return self.hexid\n",
    "    \n",
    "    def infer(self,image_id,tileids): #only bulk should be performed, there is no difference between doing 1 versus 100 tiles, so this reduces code complexity\n",
    "                                      #NOTE: there is another option here - that we accept ids from the tile table directly instead of having to accept both image_id and tile_id\n",
    "        if not self.allow_pred:\n",
    "            print(\"not doing inference --- actor was started with prediction disabled\")\n",
    "            return False\n",
    "        # probably need to *Delete* the tiles that are associated with this tileid before executing the below, so that they're not duplicated\n",
    "        # i would do that in a seperate function as \"infer\" doesn't logically mean \"delete\", so the expected behavior might be weird if it delets stuff unprompted\n",
    "        # perhaps that could be clarified with a e.g., named function paramter - or perhaps a seperate function is really needed\n",
    "        with get_session() as db_session:\n",
    "            stmt = sqlalchemy.update(Tile).where(Tile.tile_id.in_(tileids), Tile.image_id == image_id,\n",
    "                                                Tile.annotation_class_id == self.classid)\\\n",
    "                                                    .values(seen=TileStatus.STARTPROCESSING,date=sqlalchemy.func.now()) ## should add date time\n",
    "\n",
    "            # Execute the update\n",
    "            db_session.execute(stmt)\n",
    "\n",
    "        #need a similar statement to get the DL starting\n",
    "        return True\n",
    "    \n",
    "    def getTileStatus(self,image_id,tile_ids): #probably belongs else where but need this for debug\n",
    "        with get_session() as db_session:\n",
    "            stmt = db_session.query(Tile).filter(Tile.tile_id.in_(tile_ids),Tile.image_id == image_id,\n",
    "                            Tile.annotation_class_id == self.classid)\n",
    "\n",
    "            result = stmt.all()\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def getClassId(self):\n",
    "        return self.classid\n",
    "        \n",
    "    def getHexId(self):\n",
    "        return self.hexid\n",
    "\n",
    "    def setHexId(self,hexid:str): # not sure if set is smart to have -- should likely be done internally\n",
    "        self.hexid=hexid\n",
    "        return self.hexid\n",
    "\n",
    "    def setEnableTraining(self,enable_training: bool):\n",
    "        self.enable_training=enable_training\n",
    "        return self.enable_training\n",
    "    \n",
    "    def getEnableTraining(self):\n",
    "        return self.enable_training\n",
    "    \n",
    "    def getCloseDown(self):\n",
    "        return self.closedown\n",
    "    \n",
    "    def setCloseDown(self,closedown: bool):\n",
    "        self.closedown=closedown\n",
    "        return self.closedown\n",
    "    \n",
    "\n",
    "    def getActorName(self):\n",
    "        return self.actor_name\n",
    "    \n",
    "    def getTileSize(self):\n",
    "        return self.tile_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this section  is some non-ray testing to see if things work as expected. to use these, remove the @ray annotator in the actor above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classid=2\n",
    "actor_name=f\"dlactor_{classid}g\"\n",
    "print(actor_name)\n",
    "actor = DLActor.options(name=actor_name,lifetime=\"detached\",max_concurrency=2).remote(actor_name=actor_name,classid=classid,\n",
    "                                                                                      tile_size=2048,magntification=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(actor.getClassId.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(actor.getTileSize.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(actor.getActorName.remote()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(actor_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.setEnableTraining.remote(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref2=actor.start_dlproc.remote()\n",
    "print(ref2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfas!!!@3123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.setEnableTraining.remote(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "ray.get(actor.getEnableTraining.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quickannotator.db import db_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##not ray test\n",
    "class2=DLActor(classid=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2.infer(image_id=1,tileids=[323],status=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- check it was set\n",
    "tile=class2.getTileStatus([323])\n",
    "tile[0].seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref2=class2.start_dlproc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is when the @Ray actor is enabled, and thus has remote. note we need max_concurrency = 2 for this to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2 = DLActor.options(max_concurrency=2).remote(classid=2,tile_size=2_048)\n",
    "ref2=class2.start_dlproc.remote()\n",
    "print(ref2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ray.get(class2.getclassid.remote()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ray.get(class2.infer.remote(image_id=1,tileids=[323],status=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile=(ray.get(class2.getTileStatus.remote([323])))\n",
    "print(tile[0].seen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref2=class2.start_dlproc.remote()\n",
    "print(ref2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
