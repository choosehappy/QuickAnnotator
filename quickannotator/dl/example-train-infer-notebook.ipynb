{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/uv_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/uv_venv/lib/python3.10/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.5' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from quickannotator.dl.inference import run_inference, getPendingInferenceTiles\n",
    "from quickannotator.dl.dataset import TileDataset\n",
    "import io\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "def get_transforms(tile_size): #probably goes...elsewhere\n",
    "    transforms = A.Compose([\n",
    "    A.RandomScale(scale_limit=-.1, p=.1),\n",
    "    A.PadIfNeeded(min_height=tile_size, min_width=tile_size),\n",
    "    A.VerticalFlip(p=.5),\n",
    "    A.HorizontalFlip(p=.5),\n",
    "    A.Blur(p=.5),\n",
    "    # # Downscale(p=.25, scale_min=0.64, scale_max=0.99),\n",
    "    A.GaussNoise(p=.5, var_limit=(10.0, 50.0)),\n",
    "    # A.GridDistortion(p=.5, num_steps=5, distort_limit=(-0.3, 0.3),\n",
    "    #                 border_mode=cv2.BORDER_REFLECT),\n",
    "    # A.ISONoise(p=.5, intensity=(0.1, 0.5), color_shift=(0.01, 0.05)),\n",
    "    # A.RandomBrightnessContrast(p=0.5, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True),\n",
    "    # A.RandomGamma(p=.5, gamma_limit=(80, 120), eps=1e-07),\n",
    "    # A.MultiplicativeNoise(p=.5, multiplier=(0.9, 1.1), per_channel=True, elementwise=True),\n",
    "    # A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=10, val_shift_limit=10, p=.9),\n",
    "    A.Rotate(p=1, border_mode=cv2.BORDER_REFLECT),\n",
    "    A.RandomCrop(tile_size, tile_size),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalization\n",
    "    ToTensorV2()])\n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_128825/1762902026.py:27: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(p=.5, var_limit=(10.0, 50.0)),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Unet(\n",
       "  (encoder): EfficientNetEncoder(\n",
       "    (_conv_stem): Conv2dStaticSamePadding(\n",
       "      3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "      (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "    )\n",
       "    (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_blocks): ModuleList(\n",
       "      (0): MBConvBlock(\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (1): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "          (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (2): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (3): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (4): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (5): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "          (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (6-7): 2 x MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (8): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (9-10): 2 x MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (11): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (12-14): 3 x MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (15): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "    )\n",
       "    (_conv_head): Conv2dStaticSamePadding(\n",
       "      320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (static_padding): Identity()\n",
       "    )\n",
       "    (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "    (_dropout): Dropout(p=0.2, inplace=False)\n",
       "    (_swish): MemoryEfficientSwish()\n",
       "  )\n",
       "  (decoder): UnetDecoder(\n",
       "    (center): Identity()\n",
       "    (blocks): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(432, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(104, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(88, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Identity()\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classid = 2 \n",
    "tile_size = 2_048 \n",
    "\n",
    "boost_count = 5\n",
    "batch_size_train=4\n",
    "batch_size_infer=1\n",
    "edge_weight=1_000\n",
    "num_workers=0 \n",
    "\n",
    "num_iters=10\n",
    "\n",
    "dataset=TileDataset(classid, tile_size=tile_size, \n",
    "                    edge_weight=edge_weight, transforms=get_transforms(tile_size), \n",
    "                    boost_count=boost_count)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size_train, shuffle=False, num_workers=num_workers) #NOTE: for dataset of type iter - shuffle must == False\n",
    "\n",
    "model = smp.Unet(encoder_name=\"efficientnet-b0\", encoder_weights=\"imagenet\", \n",
    "                 decoder_channels=(64, 64, 64, 32, 16), in_channels=3, classes=1, encoder_freeze=True )\n",
    "\n",
    "#model = model.half()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none', ).cuda()\n",
    "optimizer = optim.NAdam(model.parameters(), lr=0.0001, weight_decay=1e-2) #TODO: this should be a setting\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder._conv_stem.weight is frozen\n",
      "encoder._bn0.weight is frozen\n",
      "encoder._bn0.bias is frozen\n",
      "encoder._blocks.0._depthwise_conv.weight is frozen\n",
      "encoder._blocks.0._bn1.weight is frozen\n",
      "encoder._blocks.0._bn1.bias is frozen\n",
      "encoder._blocks.0._se_reduce.weight is frozen\n",
      "encoder._blocks.0._se_reduce.bias is frozen\n",
      "encoder._blocks.0._se_expand.weight is frozen\n",
      "encoder._blocks.0._se_expand.bias is frozen\n",
      "encoder._blocks.0._project_conv.weight is frozen\n",
      "encoder._blocks.0._bn2.weight is frozen\n",
      "encoder._blocks.0._bn2.bias is frozen\n",
      "encoder._blocks.1._expand_conv.weight is frozen\n",
      "encoder._blocks.1._bn0.weight is frozen\n",
      "encoder._blocks.1._bn0.bias is frozen\n",
      "encoder._blocks.1._depthwise_conv.weight is frozen\n",
      "encoder._blocks.1._bn1.weight is frozen\n",
      "encoder._blocks.1._bn1.bias is frozen\n",
      "encoder._blocks.1._se_reduce.weight is frozen\n",
      "encoder._blocks.1._se_reduce.bias is frozen\n",
      "encoder._blocks.1._se_expand.weight is frozen\n",
      "encoder._blocks.1._se_expand.bias is frozen\n",
      "encoder._blocks.1._project_conv.weight is frozen\n",
      "encoder._blocks.1._bn2.weight is frozen\n",
      "encoder._blocks.1._bn2.bias is frozen\n",
      "encoder._blocks.2._expand_conv.weight is frozen\n",
      "encoder._blocks.2._bn0.weight is frozen\n",
      "encoder._blocks.2._bn0.bias is frozen\n",
      "encoder._blocks.2._depthwise_conv.weight is frozen\n",
      "encoder._blocks.2._bn1.weight is frozen\n",
      "encoder._blocks.2._bn1.bias is frozen\n",
      "encoder._blocks.2._se_reduce.weight is frozen\n",
      "encoder._blocks.2._se_reduce.bias is frozen\n",
      "encoder._blocks.2._se_expand.weight is frozen\n",
      "encoder._blocks.2._se_expand.bias is frozen\n",
      "encoder._blocks.2._project_conv.weight is frozen\n",
      "encoder._blocks.2._bn2.weight is frozen\n",
      "encoder._blocks.2._bn2.bias is frozen\n",
      "encoder._blocks.3._expand_conv.weight is frozen\n",
      "encoder._blocks.3._bn0.weight is frozen\n",
      "encoder._blocks.3._bn0.bias is frozen\n",
      "encoder._blocks.3._depthwise_conv.weight is frozen\n",
      "encoder._blocks.3._bn1.weight is frozen\n",
      "encoder._blocks.3._bn1.bias is frozen\n",
      "encoder._blocks.3._se_reduce.weight is frozen\n",
      "encoder._blocks.3._se_reduce.bias is frozen\n",
      "encoder._blocks.3._se_expand.weight is frozen\n",
      "encoder._blocks.3._se_expand.bias is frozen\n",
      "encoder._blocks.3._project_conv.weight is frozen\n",
      "encoder._blocks.3._bn2.weight is frozen\n",
      "encoder._blocks.3._bn2.bias is frozen\n",
      "encoder._blocks.4._expand_conv.weight is frozen\n",
      "encoder._blocks.4._bn0.weight is frozen\n",
      "encoder._blocks.4._bn0.bias is frozen\n",
      "encoder._blocks.4._depthwise_conv.weight is frozen\n",
      "encoder._blocks.4._bn1.weight is frozen\n",
      "encoder._blocks.4._bn1.bias is frozen\n",
      "encoder._blocks.4._se_reduce.weight is frozen\n",
      "encoder._blocks.4._se_reduce.bias is frozen\n",
      "encoder._blocks.4._se_expand.weight is frozen\n",
      "encoder._blocks.4._se_expand.bias is frozen\n",
      "encoder._blocks.4._project_conv.weight is frozen\n",
      "encoder._blocks.4._bn2.weight is frozen\n",
      "encoder._blocks.4._bn2.bias is frozen\n",
      "encoder._blocks.5._expand_conv.weight is frozen\n",
      "encoder._blocks.5._bn0.weight is frozen\n",
      "encoder._blocks.5._bn0.bias is frozen\n",
      "encoder._blocks.5._depthwise_conv.weight is frozen\n",
      "encoder._blocks.5._bn1.weight is frozen\n",
      "encoder._blocks.5._bn1.bias is frozen\n",
      "encoder._blocks.5._se_reduce.weight is frozen\n",
      "encoder._blocks.5._se_reduce.bias is frozen\n",
      "encoder._blocks.5._se_expand.weight is frozen\n",
      "encoder._blocks.5._se_expand.bias is frozen\n",
      "encoder._blocks.5._project_conv.weight is frozen\n",
      "encoder._blocks.5._bn2.weight is frozen\n",
      "encoder._blocks.5._bn2.bias is frozen\n",
      "encoder._blocks.6._expand_conv.weight is frozen\n",
      "encoder._blocks.6._bn0.weight is frozen\n",
      "encoder._blocks.6._bn0.bias is frozen\n",
      "encoder._blocks.6._depthwise_conv.weight is frozen\n",
      "encoder._blocks.6._bn1.weight is frozen\n",
      "encoder._blocks.6._bn1.bias is frozen\n",
      "encoder._blocks.6._se_reduce.weight is frozen\n",
      "encoder._blocks.6._se_reduce.bias is frozen\n",
      "encoder._blocks.6._se_expand.weight is frozen\n",
      "encoder._blocks.6._se_expand.bias is frozen\n",
      "encoder._blocks.6._project_conv.weight is frozen\n",
      "encoder._blocks.6._bn2.weight is frozen\n",
      "encoder._blocks.6._bn2.bias is frozen\n",
      "encoder._blocks.7._expand_conv.weight is frozen\n",
      "encoder._blocks.7._bn0.weight is frozen\n",
      "encoder._blocks.7._bn0.bias is frozen\n",
      "encoder._blocks.7._depthwise_conv.weight is frozen\n",
      "encoder._blocks.7._bn1.weight is frozen\n",
      "encoder._blocks.7._bn1.bias is frozen\n",
      "encoder._blocks.7._se_reduce.weight is frozen\n",
      "encoder._blocks.7._se_reduce.bias is frozen\n",
      "encoder._blocks.7._se_expand.weight is frozen\n",
      "encoder._blocks.7._se_expand.bias is frozen\n",
      "encoder._blocks.7._project_conv.weight is frozen\n",
      "encoder._blocks.7._bn2.weight is frozen\n",
      "encoder._blocks.7._bn2.bias is frozen\n",
      "encoder._blocks.8._expand_conv.weight is frozen\n",
      "encoder._blocks.8._bn0.weight is frozen\n",
      "encoder._blocks.8._bn0.bias is frozen\n",
      "encoder._blocks.8._depthwise_conv.weight is frozen\n",
      "encoder._blocks.8._bn1.weight is frozen\n",
      "encoder._blocks.8._bn1.bias is frozen\n",
      "encoder._blocks.8._se_reduce.weight is frozen\n",
      "encoder._blocks.8._se_reduce.bias is frozen\n",
      "encoder._blocks.8._se_expand.weight is frozen\n",
      "encoder._blocks.8._se_expand.bias is frozen\n",
      "encoder._blocks.8._project_conv.weight is frozen\n",
      "encoder._blocks.8._bn2.weight is frozen\n",
      "encoder._blocks.8._bn2.bias is frozen\n",
      "encoder._blocks.9._expand_conv.weight is frozen\n",
      "encoder._blocks.9._bn0.weight is frozen\n",
      "encoder._blocks.9._bn0.bias is frozen\n",
      "encoder._blocks.9._depthwise_conv.weight is frozen\n",
      "encoder._blocks.9._bn1.weight is frozen\n",
      "encoder._blocks.9._bn1.bias is frozen\n",
      "encoder._blocks.9._se_reduce.weight is frozen\n",
      "encoder._blocks.9._se_reduce.bias is frozen\n",
      "encoder._blocks.9._se_expand.weight is frozen\n",
      "encoder._blocks.9._se_expand.bias is frozen\n",
      "encoder._blocks.9._project_conv.weight is frozen\n",
      "encoder._blocks.9._bn2.weight is frozen\n",
      "encoder._blocks.9._bn2.bias is frozen\n",
      "encoder._blocks.10._expand_conv.weight is frozen\n",
      "encoder._blocks.10._bn0.weight is frozen\n",
      "encoder._blocks.10._bn0.bias is frozen\n",
      "encoder._blocks.10._depthwise_conv.weight is frozen\n",
      "encoder._blocks.10._bn1.weight is frozen\n",
      "encoder._blocks.10._bn1.bias is frozen\n",
      "encoder._blocks.10._se_reduce.weight is frozen\n",
      "encoder._blocks.10._se_reduce.bias is frozen\n",
      "encoder._blocks.10._se_expand.weight is frozen\n",
      "encoder._blocks.10._se_expand.bias is frozen\n",
      "encoder._blocks.10._project_conv.weight is frozen\n",
      "encoder._blocks.10._bn2.weight is frozen\n",
      "encoder._blocks.10._bn2.bias is frozen\n",
      "encoder._blocks.11._expand_conv.weight is frozen\n",
      "encoder._blocks.11._bn0.weight is frozen\n",
      "encoder._blocks.11._bn0.bias is frozen\n",
      "encoder._blocks.11._depthwise_conv.weight is frozen\n",
      "encoder._blocks.11._bn1.weight is frozen\n",
      "encoder._blocks.11._bn1.bias is frozen\n",
      "encoder._blocks.11._se_reduce.weight is frozen\n",
      "encoder._blocks.11._se_reduce.bias is frozen\n",
      "encoder._blocks.11._se_expand.weight is frozen\n",
      "encoder._blocks.11._se_expand.bias is frozen\n",
      "encoder._blocks.11._project_conv.weight is frozen\n",
      "encoder._blocks.11._bn2.weight is frozen\n",
      "encoder._blocks.11._bn2.bias is frozen\n",
      "encoder._blocks.12._expand_conv.weight is frozen\n",
      "encoder._blocks.12._bn0.weight is frozen\n",
      "encoder._blocks.12._bn0.bias is frozen\n",
      "encoder._blocks.12._depthwise_conv.weight is frozen\n",
      "encoder._blocks.12._bn1.weight is frozen\n",
      "encoder._blocks.12._bn1.bias is frozen\n",
      "encoder._blocks.12._se_reduce.weight is frozen\n",
      "encoder._blocks.12._se_reduce.bias is frozen\n",
      "encoder._blocks.12._se_expand.weight is frozen\n",
      "encoder._blocks.12._se_expand.bias is frozen\n",
      "encoder._blocks.12._project_conv.weight is frozen\n",
      "encoder._blocks.12._bn2.weight is frozen\n",
      "encoder._blocks.12._bn2.bias is frozen\n",
      "encoder._blocks.13._expand_conv.weight is frozen\n",
      "encoder._blocks.13._bn0.weight is frozen\n",
      "encoder._blocks.13._bn0.bias is frozen\n",
      "encoder._blocks.13._depthwise_conv.weight is frozen\n",
      "encoder._blocks.13._bn1.weight is frozen\n",
      "encoder._blocks.13._bn1.bias is frozen\n",
      "encoder._blocks.13._se_reduce.weight is frozen\n",
      "encoder._blocks.13._se_reduce.bias is frozen\n",
      "encoder._blocks.13._se_expand.weight is frozen\n",
      "encoder._blocks.13._se_expand.bias is frozen\n",
      "encoder._blocks.13._project_conv.weight is frozen\n",
      "encoder._blocks.13._bn2.weight is frozen\n",
      "encoder._blocks.13._bn2.bias is frozen\n",
      "encoder._blocks.14._expand_conv.weight is frozen\n",
      "encoder._blocks.14._bn0.weight is frozen\n",
      "encoder._blocks.14._bn0.bias is frozen\n",
      "encoder._blocks.14._depthwise_conv.weight is frozen\n",
      "encoder._blocks.14._bn1.weight is frozen\n",
      "encoder._blocks.14._bn1.bias is frozen\n",
      "encoder._blocks.14._se_reduce.weight is frozen\n",
      "encoder._blocks.14._se_reduce.bias is frozen\n",
      "encoder._blocks.14._se_expand.weight is frozen\n",
      "encoder._blocks.14._se_expand.bias is frozen\n",
      "encoder._blocks.14._project_conv.weight is frozen\n",
      "encoder._blocks.14._bn2.weight is frozen\n",
      "encoder._blocks.14._bn2.bias is frozen\n",
      "encoder._blocks.15._expand_conv.weight is frozen\n",
      "encoder._blocks.15._bn0.weight is frozen\n",
      "encoder._blocks.15._bn0.bias is frozen\n",
      "encoder._blocks.15._depthwise_conv.weight is frozen\n",
      "encoder._blocks.15._bn1.weight is frozen\n",
      "encoder._blocks.15._bn1.bias is frozen\n",
      "encoder._blocks.15._se_reduce.weight is frozen\n",
      "encoder._blocks.15._se_reduce.bias is frozen\n",
      "encoder._blocks.15._se_expand.weight is frozen\n",
      "encoder._blocks.15._se_expand.bias is frozen\n",
      "encoder._blocks.15._project_conv.weight is frozen\n",
      "encoder._blocks.15._bn2.weight is frozen\n",
      "encoder._blocks.15._bn2.bias is frozen\n",
      "encoder._conv_head.weight is frozen\n",
      "encoder._bn1.weight is frozen\n",
      "encoder._bn1.bias is frozen\n"
     ]
    }
   ],
   "source": [
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for name, param in model.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(f\"{name} is frozen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "├─EfficientNetEncoder: 1-1                              [-1, 3, 2048, 2048]       --\n",
      "|    └─Conv2dStaticSamePadding: 2-1                     [-1, 32, 1024, 1024]      --\n",
      "|    |    └─ZeroPad2d: 3-1                              [-1, 3, 2049, 2049]       --\n",
      "|    └─BatchNorm2d: 2-2                                 [-1, 32, 1024, 1024]      (64)\n",
      "|    └─MemoryEfficientSwish: 2-3                        [-1, 32, 1024, 1024]      --\n",
      "|    └─ModuleList: 2                                    []                        --\n",
      "|    |    └─MBConvBlock: 3-2                            [-1, 16, 1024, 1024]      (1,448)\n",
      "|    |    └─MBConvBlock: 3-3                            [-1, 24, 512, 512]        (6,004)\n",
      "|    |    └─MBConvBlock: 3-4                            [-1, 24, 512, 512]        (10,710)\n",
      "|    |    └─MBConvBlock: 3-5                            [-1, 40, 256, 256]        (15,350)\n",
      "|    |    └─MBConvBlock: 3-6                            [-1, 40, 256, 256]        (31,290)\n",
      "|    |    └─MBConvBlock: 3-7                            [-1, 80, 128, 128]        (37,130)\n",
      "|    |    └─MBConvBlock: 3-8                            [-1, 80, 128, 128]        (102,900)\n",
      "|    |    └─MBConvBlock: 3-9                            [-1, 80, 128, 128]        (102,900)\n",
      "|    |    └─MBConvBlock: 3-10                           [-1, 112, 128, 128]       (126,004)\n",
      "|    |    └─MBConvBlock: 3-11                           [-1, 112, 128, 128]       (208,572)\n",
      "|    |    └─MBConvBlock: 3-12                           [-1, 112, 128, 128]       (208,572)\n",
      "|    |    └─MBConvBlock: 3-13                           [-1, 192, 64, 64]         (262,492)\n",
      "|    |    └─MBConvBlock: 3-14                           [-1, 192, 64, 64]         (587,952)\n",
      "|    |    └─MBConvBlock: 3-15                           [-1, 192, 64, 64]         (587,952)\n",
      "|    |    └─MBConvBlock: 3-16                           [-1, 192, 64, 64]         (587,952)\n",
      "|    |    └─MBConvBlock: 3-17                           [-1, 320, 64, 64]         (717,232)\n",
      "├─UnetDecoder: 1-2                                      [-1, 16, 2048, 2048]      --\n",
      "|    └─Identity: 2-4                                    [-1, 320, 64, 64]         --\n",
      "|    └─ModuleList: 2                                    []                        --\n",
      "|    |    └─DecoderBlock: 3-18                          [-1, 64, 128, 128]        285,952\n",
      "|    |    └─DecoderBlock: 3-19                          [-1, 64, 256, 256]        97,024\n",
      "|    |    └─DecoderBlock: 3-20                          [-1, 64, 512, 512]        87,808\n",
      "|    |    └─DecoderBlock: 3-21                          [-1, 32, 1024, 1024]      36,992\n",
      "|    |    └─DecoderBlock: 3-22                          [-1, 16, 2048, 2048]      6,976\n",
      "├─SegmentationHead: 1-3                                 [-1, 1, 2048, 2048]       --\n",
      "|    └─Conv2d: 2-5                                      [-1, 1, 2048, 2048]       145\n",
      "|    └─Identity: 2-6                                    [-1, 1, 2048, 2048]       --\n",
      "|    └─Activation: 2-7                                  [-1, 1, 2048, 2048]       --\n",
      "|    |    └─Identity: 3-23                              [-1, 1, 2048, 2048]       --\n",
      "=========================================================================================================\n",
      "Total params: 4,109,421\n",
      "Trainable params: 514,897\n",
      "Non-trainable params: 3,594,524\n",
      "Total mult-adds (G): 31.03\n",
      "=========================================================================================================\n",
      "Input size (MB): 48.00\n",
      "Forward/backward pass size (MB): 4289.00\n",
      "Params size (MB): 15.68\n",
      "Estimated Total Size (MB): 4352.68\n",
      "=========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#_ = summary(model, (3, tile_size, tile_size),dtypes=[torch.float16]) #TODO: log this\n",
    "_ = summary(model, (3, tile_size, tile_size)) #TODO: log this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]InitSpatiaMetaData() error:\"table spatial_ref_sys already exists\"\n",
      "/opt/QuickAnnotator/quickannotator/db/utils.py:6: SAWarning: This declarative base already contains a class with the same class name and module name as quickannotator.db.utils.DynamicAnnotation, and will be replaced in the string-lookup table.\n",
      "  class DynamicAnnotation(base):\n",
      "  0%|          | 0/10 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempting to unscale FP16 gradients.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss_total)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#optimizer.step()\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/uv_venv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:451\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m OptState\u001b[38;5;241m.\u001b[39mREADY:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/uv_venv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:338\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    335\u001b[0m inv_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdouble()\u001b[38;5;241m.\u001b[39mreciprocal()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    336\u001b[0m found_inf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((), \u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 338\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unscale_grads_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mUNSCALED\n",
      "File \u001b[0;32m/opt/uv_venv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:260\u001b[0m, in \u001b[0;36mGradScaler._unscale_grads_\u001b[0;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m allow_fp16) \u001b[38;5;129;01mand\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[0;32m--> 260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to unscale FP16 gradients.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mis_sparse:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# is_coalesced() == False means the sparse grad has values with duplicate indices.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# coalesce() deduplicates indices and adds all values that have the same index.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m# For scaled fp16 values, there's a good chance coalescing will cause overflow,\u001b[39;00m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# so we should check the coalesced _values().\u001b[39;00m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "\u001b[0;31mValueError\u001b[0m: Attempting to unscale FP16 gradients."
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "running_loss = []\n",
    "writer = SummaryWriter(log_dir=f\"/tmp/{classid}/{datetime.datetime.now().strftime('%b%d_%H-%M-%S')}\")\n",
    "\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "\n",
    "for niter in tqdm(range(num_iters)): #TODO: this should be a setting\n",
    "    images, masks, weights = next(iter(dataloader))\n",
    "    #print (\"post next iter\")\n",
    "    images = images.half().to(device)\n",
    "    masks = masks.to(device)\n",
    "    weights = weights.to(device)\n",
    "    #print (\"post copy \")\n",
    "    epsilon = 1e-6\n",
    "    for _ in tqdm(range(5),leave=False):\n",
    "        \n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            outputs = model(images) \n",
    "            loss = criterion(outputs, masks.float())\n",
    "            loss = (loss * (edge_weight ** weights).type_as(loss)).mean()\n",
    "\n",
    "            positive_mask = (masks == 1).float()\n",
    "            unlabeled_mask = (masks == 0).float()\n",
    "\n",
    "            positive_loss = 1.0 * (loss * positive_mask).mean()\n",
    "            unlabeled_loss = .1 * (loss * unlabeled_mask).mean()\n",
    "\n",
    "            loss_total = positive_loss + unlabeled_loss\n",
    "        \n",
    "        #loss_total.backward()\n",
    "        scaler.scale(loss_total).backward()\n",
    "\n",
    "        #optimizer.step()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    running_loss.append(loss_total.item())\n",
    "\n",
    "    writer.add_scalar(f'loss/loss', loss, niter)\n",
    "    writer.add_scalar(f'loss/positive_loss', positive_loss, niter)\n",
    "    writer.add_scalar(f'loss/unlabeled_loss', unlabeled_loss, niter)\n",
    "    writer.add_scalar(f'loss/loss_total', loss_total, niter)\n",
    "    \n",
    "    #print (\"losses:\\t\",loss_total,positive_mask.sum(),positive_loss,unlabeled_loss)\n",
    "\n",
    "    \n",
    "    if niter % 50==0:\n",
    "        #print (f\"niter [{niter}], Loss: {sum(running_loss)/len(running_loss)}\")\n",
    "        running_loss=[]\n",
    "\n",
    "        #print (\"saving!\") #TODO: do we want to *always* override the last saved model , or do we want to instead only save if some type of loss threshold is met?\n",
    "                            #another potentially more interesting option is to do both, save on a regular basis (since if we things crash we can revert back othe nearest checkpoint\\\n",
    "                            #but as well give the user in the front end a dropdown which enables them to select which model checkpoint they want to use? we had somethng similar in QAv1\n",
    "                            #that said, this is likely a more advanced features and not very \"apple like\" since it would require explaining to the user when/why/how they should use the different models\n",
    "                            #maybe suggest avoiduing for now --- lets just save the last one\n",
    "        save_file(model.state_dict(), f\"/tmp/model_{classid}.safetensors\") #TODO: needs to go somewhere reasonable maybe /projid/models/classid/ ? or something\n",
    "        last_save = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tds=TileDataset(classid, tile_size=tile_size,\n",
    "                    edge_weight=edge_weight, transforms=None, \n",
    "                    boost_count=boost_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, masks, weights= next(iter(tds))\n",
    "plt.imshow(images)\n",
    "plt.show()\n",
    "plt.imshow(masks.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"mask2.png\",masks.squeeze()*255)\n",
    "cv2.imwrite(\"img2.png\",images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, masks, weights = next(iter(dataloader))\n",
    "i=images.cpu().detach()[0,::]\n",
    "m=masks.cpu().detach()[0,::]\n",
    "plt.imshow(i.squeeze().permute(1,2,0))\n",
    "plt.show()\n",
    "plt.imshow(m.squeeze())\n",
    "print(masks.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(masks[0,::].cpu().detach().squeeze())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(images.half().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o=torch.sigmoid(outputs.detach()).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(o[0,::].squeeze()>.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(o.squeeze().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks[0,::].cpu().detach().squeeze()*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"gt_io.png\",i.squeeze().permute(1,2,0).numpy()*128)\n",
    "\n",
    "cv2.imwrite(\"gt.png\",masks[0,::].cpu().detach().numpy().squeeze()*255)  \n",
    "cv2.imwrite(\"out.png\", (o[0,::].numpy().squeeze()>.5)*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ttach as tta\n",
    "# tta_model = tta.SegmentationTTAWrapper(model, tta.aliases.five_crop_transform(crop_height=2_016,crop_width=2_016), merge_mode='mean')\n",
    "# outputs = tta_model(images.to(device))\n",
    "# o=torch.sigmoid(outputs.detach()).cpu()\n",
    "# plt.imshow(o[0,::].squeeze()>.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from quickannotator.dl.inference import run_inference, getPendingInferenceTiles\n",
    "from quickannotator.dl.dataset import TileDataset\n",
    "import io\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "def get_transforms(tile_size): #probably goes...elsewhere\n",
    "    transforms = A.Compose([\n",
    "!    A.RandomScale(scale_limit=-0.1, p=.1),\n",
    "!    A.PadIfNeeded(min_height=tile_size, min_width=tile_size),\n",
    "!    A.VerticalFlip(p=.5),\n",
    "!    A.HorizontalFlip(p=.5),\n",
    "    # A.Blur(p=.5),\n",
    "    # # Downscale(p=.25, scale_min=0.64, scale_max=0.99),\n",
    "!    A.GaussNoise(p=.5, var_limit=(10.0, 50.0)),\n",
    "    # A.GridDistortion(p=.5, num_steps=5, distort_limit=(-0.3, 0.3),\n",
    "    #                 border_mode=cv2.BORDER_REFLECT),\n",
    "!    A.ISONoise(p=.5, intensity=(0.1, 0.5), color_shift=(0.01, 0.05)),\n",
    "    # A.RandomBrightnessContrast(p=0.5, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True),\n",
    "    # A.RandomGamma(p=.5, gamma_limit=(80, 120), eps=1e-07),\n",
    "    # A.MultiplicativeNoise(p=.5, multiplier=(0.9, 1.1), per_channel=True, elementwise=True),\n",
    "    # A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=10, val_shift_limit=10, p=.9),\n",
    "!    A.Rotate(p=1, border_mode=cv2.BORDER_REFLECT),\n",
    "!    A.RandomCrop(tile_size, tile_size),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalization\n",
    "    ToTensorV2()])\n",
    "    return transforms\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
