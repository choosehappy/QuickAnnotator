{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from quickannotator.dl.inference import run_inference, getPendingInferenceTiles\n",
    "from quickannotator.dl.dataset import TileDataset\n",
    "import io\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "def get_transforms(tile_size): #probably goes...elsewhere\n",
    "    transforms = A.Compose([\n",
    "    A.RandomScale(scale_limit=-.1, p=.1),\n",
    "    A.PadIfNeeded(min_height=tile_size, min_width=tile_size),\n",
    "    A.VerticalFlip(p=.5),\n",
    "    A.HorizontalFlip(p=.5),\n",
    "    #A.Blur(p=.5),\n",
    "    # # Downscale(p=.25, scale_min=0.64, scale_max=0.99),\n",
    "    #A.GaussNoise(p=.5, var_limit=(10.0, 50.0)),\n",
    "    # A.GridDistortion(p=.5, num_steps=5, distort_limit=(-0.3, 0.3),\n",
    "    #                 border_mode=cv2.BORDER_REFLECT),\n",
    "    # A.ISONoise(p=.5, intensity=(0.1, 0.5), color_shift=(0.01, 0.05)),\n",
    "    # A.RandomBrightnessContrast(p=0.5, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True),\n",
    "    # A.RandomGamma(p=.5, gamma_limit=(80, 120), eps=1e-07),\n",
    "    # A.MultiplicativeNoise(p=.5, multiplier=(0.9, 1.1), per_channel=True, elementwise=True),\n",
    "    # A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=10, val_shift_limit=10, p=.9),\n",
    "    A.Rotate(p=1, border_mode=cv2.BORDER_REFLECT),\n",
    "    A.RandomCrop(tile_size, tile_size),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalization\n",
    "    ToTensorV2()])\n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classid = 2 \n",
    "tile_size = 2_048 \n",
    "\n",
    "boost_count = 5\n",
    "batch_size_train=4\n",
    "batch_size_infer=1\n",
    "edge_weight=1_000\n",
    "num_workers=0 \n",
    "\n",
    "num_iters=10\n",
    "\n",
    "dataset=TileDataset(classid,\n",
    "                    edge_weight=edge_weight, transforms=get_transforms(tile_size), \n",
    "                    boost_count=boost_count)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size_train, shuffle=False, num_workers=num_workers) #NOTE: for dataset of type iter - shuffle must == False\n",
    "\n",
    "model = smp.Unet(encoder_name=\"efficientnet-b0\", encoder_weights=\"imagenet\", \n",
    "                 decoder_channels=(64, 64, 64, 32, 16), in_channels=3, classes=1) \n",
    "\n",
    "#model = model.half()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none', ).cuda()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=.01, weight_decay=1e-2) #TODO: this should be a setting\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False # \n",
    "    \n",
    "for name, param in model.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(f\"{name} is frozen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = summary(model, (3, tile_size, tile_size),dtypes=[torch.float16]) #TODO: log this\n",
    "_ = summary(model, (3, tile_size, tile_size)) #TODO: log this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "running_loss = []\n",
    "writer = SummaryWriter(log_dir=f\"/tmp/{classid}/{datetime.datetime.now().strftime('%b%d_%H-%M-%S')}\")\n",
    "\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "\n",
    "for niter in tqdm(range(num_iters)): #TODO: this should be a setting\n",
    "    images, masks, weights = next(iter(dataloader))\n",
    "    #print (\"post next iter\")\n",
    "    images = images.half().to(device)\n",
    "    masks = masks.to(device)\n",
    "    weights = weights.to(device)\n",
    "    #print (\"post copy \")\n",
    "    epsilon = 1e-6\n",
    "    for _ in tqdm(range(5),leave=False):\n",
    "        \n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            outputs = model(images) \n",
    "            loss = criterion(outputs, masks.float())\n",
    "            loss = (loss * (edge_weight ** weights).type_as(loss)).mean()\n",
    "\n",
    "            positive_mask = (masks == 1).float()\n",
    "            unlabeled_mask = (masks == 0).float()\n",
    "\n",
    "            positive_loss = 1.0 * (loss * positive_mask).mean()\n",
    "            unlabeled_loss = .1 * (loss * unlabeled_mask).mean()\n",
    "\n",
    "            loss_total = positive_loss + unlabeled_loss\n",
    "        \n",
    "        #loss_total.backward()\n",
    "        scaler.scale(loss_total).backward()\n",
    "\n",
    "        #optimizer.step()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    running_loss.append(loss_total.item())\n",
    "\n",
    "    writer.add_scalar(f'loss/loss', loss, niter)\n",
    "    writer.add_scalar(f'loss/positive_loss', positive_loss, niter)\n",
    "    writer.add_scalar(f'loss/unlabeled_loss', unlabeled_loss, niter)\n",
    "    writer.add_scalar(f'loss/loss_total', loss_total, niter)\n",
    "    \n",
    "    #print (\"losses:\\t\",loss_total,positive_mask.sum(),positive_loss,unlabeled_loss)\n",
    "\n",
    "    \n",
    "    if niter % 50==0:\n",
    "        #print (f\"niter [{niter}], Loss: {sum(running_loss)/len(running_loss)}\")\n",
    "        running_loss=[]\n",
    "\n",
    "        #print (\"saving!\") #TODO: do we want to *always* override the last saved model , or do we want to instead only save if some type of loss threshold is met?\n",
    "                            #another potentially more interesting option is to do both, save on a regular basis (since if we things crash we can revert back othe nearest checkpoint\\\n",
    "                            #but as well give the user in the front end a dropdown which enables them to select which model checkpoint they want to use? we had somethng similar in QAv1\n",
    "                            #that said, this is likely a more advanced features and not very \"apple like\" since it would require explaining to the user when/why/how they should use the different models\n",
    "                            #maybe suggest avoiduing for now --- lets just save the last one\n",
    "        save_file(model.state_dict(), f\"/tmp/model_{classid}.safetensors\") #TODO: needs to go somewhere reasonable maybe /projid/models/classid/ ? or something\n",
    "        last_save = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tds=TileDataset(classid,\n",
    "                    edge_weight=edge_weight, transforms=None, \n",
    "                    boost_count=boost_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, masks, weights= next(iter(tds))\n",
    "plt.imshow(images)\n",
    "plt.show()\n",
    "plt.imshow(masks.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"mask2.png\",masks.squeeze()*255)\n",
    "cv2.imwrite(\"img2.png\",images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, masks, weights = next(iter(dataloader))\n",
    "i=images.cpu().detach()[0,::]\n",
    "m=masks.cpu().detach()[0,::]\n",
    "plt.imshow(i.squeeze().permute(1,2,0))\n",
    "plt.show()\n",
    "plt.imshow(m.squeeze())\n",
    "print(masks.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(masks[0,::].cpu().detach().squeeze())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(images.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o=torch.sigmoid(outputs.detach()).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(o[0,::].squeeze()>.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(o.squeeze().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks[0,::].cpu().detach().squeeze()*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"gt_io.png\",i.squeeze().permute(1,2,0).numpy()*128)\n",
    "\n",
    "cv2.imwrite(\"gt.png\",masks[0,::].cpu().detach().numpy().squeeze()*255)  \n",
    "cv2.imwrite(\"out.png\", (o[0,::].numpy().squeeze()>.5)*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ttach as tta\n",
    "# tta_model = tta.SegmentationTTAWrapper(model, tta.aliases.five_crop_transform(crop_height=2_016,crop_width=2_016), merge_mode='mean')\n",
    "# outputs = tta_model(images.to(device))\n",
    "# o=torch.sigmoid(outputs.detach()).cpu()\n",
    "# plt.imshow(o[0,::].squeeze()>.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
